
@InProceedings{pmlr-v202-mao23b,
  title = 	 {Cross-Entropy Loss Functions: Theoretical Analysis and Applications},
  author =       {Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {23803--23828},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/mao23b/mao23b.pdf},
  url = 	 {https://proceedings.mlr.press/v202/mao23b.html},
  abstract = 	 {Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of loss functions, <em>comp-sum losses</em>, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other cross-entropy-like loss functions. We give the first $H$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set $H$ used. We further show that our bounds are <em>tight</em>. These bounds depend on quantities called <em>minimizability gaps</em>. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduce a new family of loss functions, <em>smooth adversarial comp-sum losses</em>, that are derived from their comp-sum counterparts by adding in a related smooth term. We show that these loss functions are beneficial in the adversarial setting by proving that they admit $H$-consistency bounds. This leads to new adversarial robustness algorithms that consist of minimizing a regularized smooth adversarial comp-sum loss. While our main purpose is a theoretical analysis, we also present an extensive empirical analysis comparing comp-sum losses. We further report the results of a series of experiments demonstrating that our adversarial robustness algorithms outperform the current state-of-the-art, while also achieving a superior non-adversarial accuracy.}
}
@article{aly2005survey,
  title={Survey on multiclass classification methods},
  author={Aly, Mohamed},
  journal={Neural Netw},
  volume={19},
  number={1-9},
  pages={2},
  year={2005},
  publisher={Citeseer}
}
@book{breiman2017classification,
  title={Classification and regression trees},
  author={Breiman, Leo},
  year={2017},
  publisher={Routledge}
}
@inproceedings{bay1998combining,
  title={Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets.},
  author={Bay, Stephen D},
  booktitle={ICML},
  volume={98},
  pages={37--45},
  year={1998},
  organization={Citeseer}
}
@article{brereton2010support,
  title={Support vector machines for classification and regression},
  author={Brereton, Richard G and Lloyd, Gavin R},
  journal={Analyst},
  volume={135},
  number={2},
  pages={230--267},
  year={2010},
  publisher={Royal Society of Chemistry}
}
@article{zhou2021convergence,
  title={Convergence of stochastic gradient descent in deep neural network},
  author={Zhou, Bai-cun and Han, Cong-ying and Guo, Tian-de},
  journal={Acta Mathematicae Applicatae Sinica, English Series},
  volume={37},
  number={1},
  pages={126--136},
  year={2021},
  publisher={Springer}
}

@Article{math11112466,
AUTHOR = {Abdulkadirov, Ruslan and Lyakhov, Pavel and Nagornov, Nikolay},
TITLE = {Survey of Optimization Algorithms in Modern Neural Networks},
JOURNAL = {Mathematics},
VOLUME = {11},
YEAR = {2023},
NUMBER = {11},
ARTICLE-NUMBER = {2466},
URL = {https://www.mdpi.com/2227-7390/11/11/2466},
ISSN = {2227-7390},
ABSTRACT = {The main goal of machine learning is the creation of self-learning algorithms in many areas of human activity. It allows a replacement of a person with artificial intelligence in seeking to expand production. The theory of artificial neural networks, which have already replaced humans in many problems, remains the most well-utilized branch of machine learning. Thus, one must select appropriate neural network architectures, data processing, and advanced applied mathematics tools. A common challenge for these networks is achieving the highest accuracy in a short time. This problem is solved by modifying networks and improving data pre-processing, where accuracy increases along with training time. Bt using optimization methods, one can improve the accuracy without increasing the time. In this review, we consider all existing optimization algorithms that meet in neural networks. We present modifications of optimization algorithms of the first, second, and information-geometric order, which are related to information geometry for Fisherâ€“Rao and Bregman metrics. These optimizers have significantly influenced the development of neural networks through geometric and probabilistic tools. We present applications of all the given optimization algorithms, considering the types of neural networks. After that, we show ways to develop optimization algorithms in further research using modern neural networks. Fractional order, bilevel, and gradient-free optimizers can replace classical gradient-based optimizers. Such approaches are induced in graph, spiking, complex-valued, quantum, and wavelet neural networks. Besides pattern recognition, time series prediction, and object detection, there are many other applications in machine learning: quantum computations, partial differential, and integrodifferential equations, and stochastic processes.},
DOI = {10.3390/math11112466}
}
@InProceedings{He_2015_ICCV,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}
@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}