{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ed1c0c-db1b-4083-851d-4630feb4eac1",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "***\n",
    "## Data Collection\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff417034-d2a4-4908-982e-6550da2778cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load RawData\n",
    "rawData = pd.read_csv(\"../Data/Almond.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57960059-0bdf-4247-8c64-9788d71215af",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Vizualization\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95bd395-53a9-4722-bfa4-b8cc55d80bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_boxplot(column, data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=data[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7a769-ee36-4e38-bf88-2e034b6cbb2c",
   "metadata": {},
   "source": [
    "***\n",
    "## Imputation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977c5b9-628a-463b-8445-a28ce3560a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sklearn libraries\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Retrieve Length, Width and Thickness for imputation\n",
    "# Aswell as Area\n",
    "# rawData_no_outliers.reset_index(drop=True, inplace=True)\n",
    "p_LWTA = rawData[['Length (major axis)','Width (minor axis)','Thickness (depth)','Area']].copy()\n",
    "# Set Area to NaN where length is NaN\n",
    "p_LWTA['Area'] = np.where(p_LWTA['Length (major axis)'].notna(),\n",
    "                          p_LWTA['Area'],\n",
    "                          np.nan)\n",
    "\n",
    "# Use iterative imputation using sklearn\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "d_LWTA_imputed = pd.DataFrame(imputer.fit_transform(p_LWTA), columns=p_LWTA.columns)\n",
    "\n",
    "# Calculate Roundness using the imputed Area when there is length\n",
    "d_LWTA_imputed['Roundness'] = 4 * d_LWTA_imputed['Area'] / (np.pi * d_LWTA_imputed['Length (major axis)']**2)\n",
    "\n",
    "# Remove irrelavent features\n",
    "p_proc = rawData.drop(columns=['Id']).copy()\n",
    "p_proc[['Length (major axis)','Width (minor axis)','Thickness (depth)','Roundness']] = d_LWTA_imputed[['Length (major axis)','Width (minor axis)','Thickness (depth)','Roundness']]\n",
    "p_proc['Aspect Ratio'] = p_proc['Length (major axis)']/p_proc['Width (minor axis)']\n",
    "p_proc['Eccentricity'] = (1 - (p_proc['Width (minor axis)']/p_proc['Length (major axis)'])**2) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea7e29-ac0c-4240-9647-ee6452036f3d",
   "metadata": {},
   "source": [
    "***\n",
    "## Handling Potential Bias\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b49bc-a747-4fcf-966a-a05cfd74914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee916285-d5f4-4b47-be70-ad33f687bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = p_proc[['Length (major axis)', 'Width (minor axis)', 'Thickness (depth)', 'Area', 'Perimeter', 'Roundness', 'Solidity', 'Compactness', 'Aspect Ratio', 'Eccentricity', 'Extent', 'Convex hull(convex area)']]\n",
    "Y = p_proc['Type']\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, Y)\n",
    "\n",
    "X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "y_resampled = pd.Series(y_resampled, name='Type')\n",
    "\n",
    "print(X_resampled.shape)\n",
    "print(y_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143ef03-f8f9-4ad6-824a-ddb9e9c91d8c",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Split\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8f496-2e56-4f0a-ac7a-dfc2fe645839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for NN\n",
    "# Pretty sure this shit is just magic\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_resampled.reset_index(drop=True, inplace=True)\n",
    "y_resampled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Label encoding (Not Really?)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# X,Y -> X_tensor,Y_tensor\n",
    "X_tensor = torch.tensor(X_resampled.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(label_encoder.fit_transform(y_resampled), dtype=torch.long)\n",
    "\n",
    "# Splitting Dataset into training, validation, and testing sets\n",
    "# Train + (Val | Test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_tensor, \n",
    "    y_tensor, \n",
    "    test_size=0.3, \n",
    "    stratify=y_tensor, \n",
    "    random_state=21\n",
    ")\n",
    "\n",
    "# Val + Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, \n",
    "    y_temp, \n",
    "    test_size=1/3, \n",
    "    stratify=y_temp, \n",
    "    random_state=69\n",
    ")\n",
    "\n",
    "# Check the distribution of classes in each set\n",
    "print(\"Training set class distribution:\\n\", pd.Series(y_train).value_counts(normalize=True))\n",
    "print(\"Validation set class distribution:\\n\", pd.Series(y_val).value_counts(normalize=True))\n",
    "print(\"Testing set class distribution:\\n\", pd.Series(y_test).value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# Apply training data transformation across all three sets for consistency\n",
    "mean = X_train.mean(dim=0)\n",
    "std = X_train.std(dim=0)\n",
    "\n",
    "X_train_norm = (X_train - mean)/std\n",
    "X_val_norm = (X_val - mean)/std\n",
    "X_test_norm = (X_test - mean)/std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed260b-6865-42ec-a433-79af59a2e68a",
   "metadata": {},
   "source": [
    "***\n",
    "## Neural Network Definitions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc061537-cc30-4b7a-b834-7791974a1ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as Func\n",
    "import torch.nn.init as init\n",
    "\n",
    "class FirstLayer(nn.Module):\n",
    "    def __init__(self, output_size, activation='relu', init_method='he'):\n",
    "        super(FirstLayer, self).__init__()\n",
    "        self.fc = nn.Linear(12, output_size)\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "        if init_method == 'he':\n",
    "            init.kaiming_normal_(self.fc.weight, nonlinearity='relu')\n",
    "        elif init_method == 'xavier':\n",
    "            init.xavier_normal_(self.fc.weight)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported initialization method: {init_method}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(OutputLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class ReluLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ReluLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        init.kaiming_normal_(self.fc.weight, nonlinearity='relu')\n",
    "        self.out_features = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class LeakyReluLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, neg_slope=0.01):\n",
    "        super(LeakyReluLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=neg_slope)\n",
    "        init.kaiming_normal_(self.fc.weight, nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class PReLULayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PReLULayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.PReLU()\n",
    "        init.kaiming_normal_(self.fc.weight, nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class SigmoidLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SigmoidLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class SwishLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SwishLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class DynamicNN(nn.Module):\n",
    "    def __init__(self, no_hidden_layers, first_layer_neurons, alpha):\n",
    "        super(DynamicNN, self).__init__()\n",
    "        self.in_layer = FirstLayer(output_size=first_layer_neurons)\n",
    "        hidden_layers = []\n",
    "        \n",
    "        current_neurons = first_layer_neurons\n",
    "        \n",
    "        for i in range(no_hidden_layers):\n",
    "            if i > 0:\n",
    "                current_neurons = max(int(first_layer_neurons * (1 - alpha * i)), 3)\n",
    "                hidden_layers.append(ReluLayer(input_size=hidden_layers[-1].out_features, output_size=current_neurons))\n",
    "            else:\n",
    "                hidden_layers.append(ReluLayer(input_size=first_layer_neurons, output_size=current_neurons))\n",
    "        \n",
    "        \n",
    "        self.out = OutputLayer(input_size=current_neurons)\n",
    "\n",
    "        self.model = nn.Sequential(*hidden_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.in_layer(x)\n",
    "        x = self.model(x)\n",
    "        return self.out(x)\n",
    "        \n",
    "    def print_structure(self):\n",
    "        print(\"Input Layer: \", self.in_layer)\n",
    "        for i, layer in enumerate(self.model):\n",
    "            print(f\"Hidden Layer {i + 1}: {layer}\")\n",
    "        print(\"Output Layer: \", self.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7685c-d3c7-483b-8bd1-16af92058080",
   "metadata": {},
   "source": [
    "***\n",
    "## Helpers\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346c1e2-ca83-4c40-8916-993b138b03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TrainingAlgo(Enum):\n",
    "    ADAM = 0        # Adam\n",
    "    RPROP = 1       # Resilient Backpropagation\n",
    "    SGD = 2         # Schocastic Gradient Decent\n",
    "\n",
    "class ObjectiveFunc(Enum):\n",
    "    CEL = 0         # CrossEntropyLoss\n",
    "    BCEWLL = 1      # BCEWithLogitsLoss\n",
    "\n",
    "def learningAlgo(opt: TrainingAlgo):\n",
    "    if opt == TrainingAlgo.ADAM:\n",
    "        return optim.Adam\n",
    "    elif opt == TrainingAlgo.RPROP:\n",
    "        return optim.Rprop\n",
    "    else:\n",
    "        return optim.SGD\n",
    "\n",
    "def objectiveFunc(opt: ObjectiveFunc):\n",
    "    if opt == ObjectiveFunc.CEL:\n",
    "        return nn.CrossEntropyLoss()\n",
    "    elif opt == ObjectiveFunc.BCEWLL:\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "def convert_tensor(X,Y,batch,shuffle):\n",
    "    return torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X, Y), batch_size=batch, shuffle=shuffle)\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "    \n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return total_loss / len(val_loader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be6e04-9cf5-4c49-8a13-8b405601a352",
   "metadata": {},
   "source": [
    "***\n",
    "## NN Architecture Grid Search using K-Fold cross Validation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f579c0-0b2d-4833-a2e5-45a629c89f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def grid_search_k_fold(X, y, hidden_layers_options, neurons_options, alpha, k=5):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for hidden_layers in hidden_layers_options:\n",
    "        for first_layer_neurons in neurons_options:\n",
    "            fold_accuracies = []\n",
    "            fold_losses = []\n",
    "\n",
    "            model = DynamicNN(no_hidden_layers=hidden_layers, first_layer_neurons=first_layer_neurons, alpha=alpha)\n",
    "            optimizer = learningAlgo(learning_opt)(model.parameters(),lr=learning_rate)\n",
    "            criterion = objectiveFunc(objective_opt)\n",
    "\n",
    "            model.print_structure()\n",
    "            \n",
    "            for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "                print(f'Fold {fold + 1}/{k}')\n",
    "                X_train, X_val = X[train_index], X[val_index]\n",
    "                y_train, y_val = y[train_index], y[val_index]\n",
    "                \n",
    "                mean = X_train.mean(dim=0)\n",
    "                std = X_train.std(dim=0)\n",
    "                \n",
    "                X_train_norm = (X_train - mean)/std\n",
    "                X_val_norm = (X_val - mean)/std\n",
    "\n",
    "                train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_norm, y_train), batch_size=32, shuffle=True)\n",
    "                val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val_norm, y_val), batch_size=32, shuffle=False)\n",
    "\n",
    "                for epoch in range(100):\n",
    "                    train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "                loss, accuracy = evaluate_model(model, val_loader, criterion)\n",
    "                fold_losses.append(loss)\n",
    "                fold_accuracies.append(accuracy)\n",
    "\n",
    "            mean_accuracy = np.mean(fold_accuracies)\n",
    "            mean_loss = np.mean(fold_losses)\n",
    "            results.append({\n",
    "                'hidden_layers': hidden_layers,\n",
    "                'first_layer_neurons': first_layer_neurons,\n",
    "                'mean_accuracy': mean_accuracy,\n",
    "                'mean_loss': mean_loss,\n",
    "                'std_accuracy': np.std(fold_accuracies),\n",
    "                'std_loss': np.std(fold_losses),\n",
    "            })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef01803-d826-447e-becb-d557e6c6d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers_options = [1, 2, 3, 4, 5, 6]\n",
    "neurons_options = [12, 11, 10, 9, 8]\n",
    "alpha = 0.25\n",
    "\n",
    "results = grid_search_k_fold(X_tensor, y_tensor, hidden_layers_options, neurons_options, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54694189-7b17-4738-b5c3-f8da63a81672",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Pivot the DataFrame to create a matrix for the heatmap\n",
    "heatmap_data = results_df.pivot(index='hidden_layers', columns='first_layer_neurons', values='mean_accuracy')\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='viridis', cbar_kws={'label': 'Performance Metric'})\n",
    "plt.title('Grid Search Results Heatmap')\n",
    "plt.xlabel('Number of neurons in the first Layer')\n",
    "plt.ylabel('Number of Hidden Layers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de6b1d-b867-441c-b50f-8d6a9d09c9ee",
   "metadata": {},
   "source": [
    "***\n",
    "## Training Algorithms\n",
    "***\n",
    "### Shared Hyperparameters\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34261e-40ba-459f-874a-4da360e79afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate  =0.005\n",
    "num_epochs = 300\n",
    "batch_size = 34\n",
    "objective_opt = ObjectiveFunc.CEL\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f6fc8-5128-426a-8f10-c1685993814d",
   "metadata": {},
   "source": [
    "***\n",
    "## Adam\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12f2cd-414c-4c78-8c64-f85f4965f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_opt = TrainingAlgo.ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2fb51f-85b4-40c6-acd2-2eed1ddff5b0",
   "metadata": {},
   "source": [
    "***\n",
    "## RProp\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2a1d6-a444-4859-8d22-f31e8050b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_opt = TrainingAlgo.RPROP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fad2b8-4592-4694-a166-111189baf05c",
   "metadata": {},
   "source": [
    "***\n",
    "## SGD\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea9310-b328-43f1-8107-4784e28c6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_opt = TrainingAlgo.SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187617a-ae43-4af1-9ea1-01f84188155d",
   "metadata": {},
   "source": [
    "***\n",
    "## Hybrid\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd0ffbc-da89-4dc4-899e-658358879bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
