{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea438307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "# Pandas for data preparation and Numpty for DP logic\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5b91ef-324a-449c-af88-0e5537fbaabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RawData\n",
    "rawData = pd.read_csv(\"../Data/Almond.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed1c0c-db1b-4083-851d-4630feb4eac1",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "## Multiple Imputation\n",
    "Since there is a large amount of missing data with:\n",
    "- 31% missing Length\n",
    "- 34% missing Width\n",
    "- 36% missing Thickness\n",
    "Simple median or mean imputation will not due, hence we shall use multiple imputation for these 3 attributes.\n",
    "## Derived Attributes\n",
    "The attributes of Aspect Ratio and Eccentricity are derived from length and width and are missing where either length or width is missing and so we can calculate these attributes using the new imputed values.\n",
    "## Roundness Exception\n",
    "Roundness is different from the other derived attributes in that it is calculated from both Area and Length. Since area is obviously affected by the profile taken of the almond (Top/Side/Front) we cannot simply interpret roundness without "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7941c62d-ee2e-4a8f-adaa-df78165d7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Length, Width and Thickness for imputation\n",
    "p_LWT = rawData[['Length (major axis)','Width (minor axis)','Thickness (depth)','Area']].copy()\n",
    "\n",
    "p_LWT['Area'] = np.where(p_LWT['Length (major axis)'].notna(),\n",
    "                          p_LWT['Area'],\n",
    "                          np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d604b9d-c98d-4c03-a656-1f82ae6671d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sklearn for multiple imputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "# For binary one-hot encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# For testing and training split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae736f6c-d92c-4ee5-8a6a-6ee600a00b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multiple imputation using sklearn\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "d_LWT_imputed = pd.DataFrame(imputer.fit_transform(p_LWT), columns=p_LWT.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14504d4-9464-46ff-8a25-b00c1dd695e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Roundness using the imputed Area when there is length\n",
    "d_LWT_imputed['Roundness'] = 4 * d_LWT_imputed['Area'] / (np.pi * d_LWT_imputed['Length (major axis)']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95088790-5336-4179-8ca4-55f0aae35510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelavent features\n",
    "p_proc = rawData.drop('Id',axis=1)\n",
    "# Use imputed data to calculate derived features\n",
    "p_proc[['Length (major axis)','Width (minor axis)','Thickness (depth)','Roundness']] = d_LWT_imputed[['Length (major axis)','Width (minor axis)','Thickness (depth)','Roundness']]\n",
    "p_proc['Aspect Ratio'] = p_proc['Length (major axis)']/p_proc['Width (minor axis)']\n",
    "p_proc['Eccentricity'] = (1 - (p_proc['Width (minor axis)']/p_proc['Length (major axis)'])**2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442fc264-14b6-4aae-93a4-9217431baca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "p_norm = p_proc[['Length (major axis)','Width (minor axis)','Thickness (depth)','Area','Perimeter','Roundness','Solidity','Compactness','Aspect Ratio','Eccentricity','Extent','Convex hull(convex area)']]\n",
    "p_norm = (p_norm - p_norm.mean()) / p_norm.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7002215c-64cf-458b-8db7-d78adab3f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary One Hot Encoding\n",
    "labeler = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd14f730-99a0-4fca-aaf0-95b1eb292d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "X = p_norm\n",
    "# Target\n",
    "Y = p_proc['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb531e2-5aec-445c-846e-5235a02a1f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1cc5837-ed14-43e0-8d3c-047396822fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(labeler.fit_transform(Y), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6393f26f-3bc4-4ea4-8473-75f2a5e72300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed260b-6865-42ec-a433-79af59a2e68a",
   "metadata": {},
   "source": [
    "# Neural Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cecdf33c-fc0e-457a-a8ff-4e39934fadda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NathansWeirdNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NathansWeirdNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_tensor.shape[1], 64)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(64, 32)                 # Second hidden layer\n",
    "        self.fc3 = nn.Linear(32, 3)                  # Output layer (3 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c32df01-adc6-4d46-852e-cc68f80f4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NathansWeirdNN()\n",
    "criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a532278-672d-4e4d-a23c-9d86161d8797",
   "metadata": {},
   "source": [
    "## Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "538763d8-0291-4463-bc2a-0e8fa4842435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0756\n",
      "Epoch [20/100], Loss: 1.0352\n",
      "Epoch [30/100], Loss: 0.9903\n",
      "Epoch [40/100], Loss: 0.9494\n",
      "Epoch [50/100], Loss: 0.9160\n",
      "Epoch [60/100], Loss: 0.8876\n",
      "Epoch [70/100], Loss: 0.8613\n",
      "Epoch [80/100], Loss: 0.8354\n",
      "Epoch [90/100], Loss: 0.8096\n",
      "Epoch [100/100], Loss: 0.7842\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100  # Number of epochs to train\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train.long())  # Ensure y_train is of type LongTensor for classification\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3921df2-15fd-4d2e-b7a6-2d27e233551f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
